# Model Training Documentation
## Iterations
1. For the first iteration, I used the default MFCC and Keras Classifier parameters in EdgeImpulse, with roughly 200 samples of "excuse me" from myself, 200 samples of "qingwen", 200 random noise samples from EdgeImpulse, and 200 random English words also from Edge Impulse. This resulted in too-good-to-be-true results for the first iteration (92.2% accuracy overall)
2. In the second iteration, I added noise in the Data Augmentation section of Edge Impulse: all categories were set to high, except for "Warp Time Axis" which was left off. Accuracy was maintained at 92.8%.
3. Realising this accuracy may be misrepresentative among real-time use cases (in which English-as-a-second-language speakers would also be saying excuse me) I added in 9 Excuse Me's from other speaker with slightly different accents. On regenerating features, several of the new excuse me's were segmented closer to "qingwen" than to "excuse me". Auto-balancing was also ticked in this round to reduce overfitting, given the small dataset sizes.
4. With "qingwen" only accurately classified 91% of time (much lower than "excuse me"), I added in more "qingwen" data by adding in a 3 minutes more of data from one more speaker. Unfortunately, Edge Impulse was unable to process this additional data.
5. I was concerned that the fact that the fact that non-qingwen Mandarin dialogue would skew the results, so I added in 4 minutes of Mandarin "unknown" dialogue from 4 minutes of podcast recording. After cropping, this was reduced to 1:54 of one second samples. Again, the Out of Memory error occurred.
6. I tried disabling the Mandarin unknown dataset, but the Out of Memory error continued. However, by deleting the data, I was able to remove the out of memory error. I found that the total data limit before an out of memory error occurred in Edge Impulse was roughly 13 minutes.
7. Note: this iteration was performed in the [Excuse me alone project]() I wanted to experiment with how well the wake word recogniser would work with only using excuse me as a wake word, while replacing the qingwen dataset with the Mandarin unknown dataset. In total, I uploaded 3 minutes of 1-second samples from three different podcasts. Again, I experienced an Out of Memory error, but exporting data and uploading it to a new model fixed the issue. This iteration had a very low accuracy between "Mandarin unknown" and "English unknown"
8. I combined "Mandarin unknown" with "English unknown" into a single "unknown" label and reintroduced "Qingwen" as a separate label. 6 Minutes total of Qingwen data was introduced, knowing that it would likely reduce substantially in the clipping process. Too many "Qingwen" samples were misclassified.
9. At this iteration, I reclassified the incorrectly labelled data samples and moved them to the training set. This led to some overfitting, especially among the "qingwen" data.
10. To reduce the overfitting, I dropped the learning rate from .005 to .0025. This dropped the model accuracy to 80%, but the model testing accuracy went up significantly. This was the first model I used for building the end-user application.
11. In iteration 11, I tried out Autobalancing, however this had negligible effect so I turned it off again.
12. At iteration 12, I added a little bit of data augmentation. Specifically, I noise to low and masked time bands low. This significantly improved qingwen results but led many "excuse mes" to be misclassified as "qingwens", an inconsequential error in the Command Line app I developed.
13. I tried increasing training cycles to 200 from 100. This led to substantial overfitting.
14. I increased augmentation to high for all of the categories, to see if that would counterbalance the overfitting from the additional cycles.
15. I tried out the 2D convolutional network architecture to see if there was any benefit. Overfitting was still taking place.
16. I reduced the number of cycles back down to 100, to compare the performance to that of the 1D convolutional architecture.
17. I used what was recommended by the EON tuner tool after evaluating the confusion matrix that resulted from its most accurate model.